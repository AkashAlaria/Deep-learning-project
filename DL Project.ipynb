{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ecd590",
   "metadata": {},
   "source": [
    "# Loading the Datasets\n",
    "\n",
    "We need to load both CSV files and label them appropriately (e.g., 1 for fake news and 0 for real news)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e096ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n",
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "fake_df = pd.read_csv('/Users/sora/Downloads/Fake.csv')\n",
    "true_df = pd.read_csv('/Users/sora/Downloads/True.csv')\n",
    "\n",
    "# Preview the datasets\n",
    "print(fake_df.head())\n",
    "print(true_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e0bdb",
   "metadata": {},
   "source": [
    "Both datasets contain a text column with the articles, we can create labels for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dcb795",
   "metadata": {},
   "source": [
    "# Labeling the Datasets\n",
    "\n",
    "Add a label column to each dataset to differentiate fake and real news. For example, 1 for fake news and 0 for real news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9053b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a label column: 1 for fake news, 0 for real news\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ee341",
   "metadata": {},
   "source": [
    "# Combining the Datasets\n",
    "\n",
    "Once labeled, we can combine both datasets into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34672a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   While We Were Sleeping, Trump Declared Civil War   \n",
      "1  Iraqi forces complete Kirkuk province takeover...   \n",
      "2   Mike Pence’s DC Neighbors Just Invited Him To...   \n",
      "3  UK counter-terrorism police arrest 11 in far-r...   \n",
      "4  TRUMP CHALLENGES FAKE MEDIA: “Are we going to ...   \n",
      "\n",
      "                                                text    subject  \\\n",
      "0  I woke up this morning completely unfazed over...       News   \n",
      "1  BAGHDAD/ERBIL, Iraq (Reuters) - Iraqi forces o...  worldnews   \n",
      "2  A group of Vice President-elect Mike Pence s n...       News   \n",
      "3  LONDON (Reuters) - British police said 11 peop...  worldnews   \n",
      "4  You have to give it to President Trump who wen...  left-news   \n",
      "\n",
      "                  date  label  \n",
      "0     January 25, 2017      1  \n",
      "1    October 20, 2017       0  \n",
      "2    December 13, 2016      1  \n",
      "3  September 27, 2017       0  \n",
      "4         Aug 15, 2017      1  \n",
      "label\n",
      "1    23481\n",
      "0    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine the datasets\n",
    "df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset to mix fake and real news\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the structure of the combined dataset\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())  # Check the distribution of fake and real news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbce97",
   "metadata": {},
   "source": [
    "# Cleaning the Dataset\n",
    "\n",
    "Now that the data is combined, we can proceed with cleaning by removing missing values and any irrelevant columns (if present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d8ea4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n",
      "                                                text    subject  label\n",
      "0  I woke up this morning completely unfazed over...       News      1\n",
      "1  BAGHDAD/ERBIL, Iraq (Reuters) - Iraqi forces o...  worldnews      0\n",
      "2  A group of Vice President-elect Mike Pence s n...       News      1\n",
      "3  LONDON (Reuters) - British police said 11 peop...  worldnews      0\n",
      "4  You have to give it to President Trump who wen...  left-news      1\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in the 'text' column\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# If any unnecessary columns are present, you can drop them (e.g., 'title', 'date')\n",
    "if 'title' in df.columns:\n",
    "    df = df.drop(['title', 'date'], axis=1)\n",
    "\n",
    "# Final structure check\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effc2c8b",
   "metadata": {},
   "source": [
    "# Preprocessing the Text Data\n",
    "\n",
    "Next, we’ll preprocess the text data, including tokenization, normalization, and padding.\n",
    "\n",
    "## a. Preprocessing Text\n",
    "\n",
    "Normalize the text by removing punctuation, stopwords, and converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b11b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/sora/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    woke morning completely unfazed idea donald tr...\n",
      "1    baghdaderbil iraq reuters iraqi forces friday ...\n",
      "2    group vice presidentelect mike pence new washi...\n",
      "3    london reuters british police said 11 people a...\n",
      "4    give president trump went fake news media big ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocess the text: lowercase, remove punctuation and stopwords\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Check an example after preprocessing\n",
    "print(df['text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0f050",
   "metadata": {},
   "source": [
    "## b. Tokenization and Padding\n",
    "\n",
    "Tokenize the preprocessed text and pad the sequences to ensure all articles are of the same length for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538d3a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences shape: (44898, 500)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define maximum vocabulary size and sequence length\n",
    "vocab_size = 10000\n",
    "maxlen = 500\n",
    "\n",
    "# Initialize the tokenizer and fit it on the combined text data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "\n",
    "# Convert the news articles to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
    "\n",
    "# Pad the sequences to ensure uniform length\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84d5b2",
   "metadata": {},
   "source": [
    "# Preparing the Labels\n",
    "\n",
    "You can extract the labels into a numpy array for model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05952e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels shape: (44898,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = df['label'].values\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c8c38",
   "metadata": {},
   "source": [
    "# Splitting the Dataset for Training and Testing\n",
    "\n",
    "Next, split the data into training and test sets to evaluate the model later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6398ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (35918, 500), Training labels shape: (35918,)\n",
      "Test data shape: (8980, 500), Test labels shape: (8980,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the training and test sets\n",
    "print(f\"Training data shape: {x_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}, Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3fa87",
   "metadata": {},
   "source": [
    "# Saving the Preprocessed Dataset\n",
    "\n",
    "Finally, save the cleaned and preprocessed dataset for later use in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840c68e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Fake News Detection dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the preprocessed dataset as a binary file\n",
    "with open('fake_news_preprocessed_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump((x_train, y_train, x_test, y_test), f)\n",
    "\n",
    "print(\"Preprocessed Fake News Detection dataset saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352ebcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
